{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-18T15:54:33.808238Z","iopub.status.busy":"2023-04-18T15:54:33.807839Z","iopub.status.idle":"2023-04-18T15:54:33.820688Z","shell.execute_reply":"2023-04-18T15:54:33.814159Z","shell.execute_reply.started":"2023-04-18T15:54:33.808202Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import math\n","import os\n","import pandas as pd\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:54:33.825153Z","iopub.status.busy":"2023-04-18T15:54:33.824719Z","iopub.status.idle":"2023-04-18T15:54:33.835480Z","shell.execute_reply":"2023-04-18T15:54:33.833966Z","shell.execute_reply.started":"2023-04-18T15:54:33.825107Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:54:33.838924Z","iopub.status.busy":"2023-04-18T15:54:33.838359Z","iopub.status.idle":"2023-04-18T15:54:33.846653Z","shell.execute_reply":"2023-04-18T15:54:33.845387Z","shell.execute_reply.started":"2023-04-18T15:54:33.838877Z"},"trusted":true},"outputs":[],"source":["config = {\n","    \"num_labels\": 7,\n","    \"hidden_dropout_prob\": 0.15,\n","    \"hidden_size\": 768,\n","    \"max_length\": 512,\n","}\n","\n","training_parameters = {\n","    \"batch_size\": 2,\n","    \"epochs\": 1,\n","    \"output_folder\": \"/kaggle/working\",\n","    \"output_file\": \"model.bin\",\n","    \"learning_rate\": 2e-5,\n","    \"print_after_steps\": 100,\n","    \"save_steps\": 5000,\n","\n","}"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:54:33.849236Z","iopub.status.busy":"2023-04-18T15:54:33.848434Z","iopub.status.idle":"2023-04-18T15:54:33.860494Z","shell.execute_reply":"2023-04-18T15:54:33.859454Z","shell.execute_reply.started":"2023-04-18T15:54:33.849198Z"},"trusted":true},"outputs":[],"source":["# !pip install transformers"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:58:26.708832Z","iopub.status.busy":"2023-04-18T15:58:26.708155Z","iopub.status.idle":"2023-04-18T15:58:28.003654Z","shell.execute_reply":"2023-04-18T15:58:28.002636Z","shell.execute_reply.started":"2023-04-18T15:58:26.708793Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'AutoModel' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32me:\\Work_DatPT\\Study\\Master\\SecBERT\\GAN_exp.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Work_DatPT/Study/Master/SecBERT/GAN_exp.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mjackaduma/SecBERT\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'AutoModel' is not defined"]}],"source":["model = AutoModel.from_pretrained('jackaduma/SecBERT')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:04:01.425629Z","iopub.status.busy":"2023-04-18T16:04:01.424985Z","iopub.status.idle":"2023-04-18T16:04:02.056763Z","shell.execute_reply":"2023-04-18T16:04:02.055717Z","shell.execute_reply.started":"2023-04-18T16:04:01.425585Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained('jackaduma/SecBERT')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:22.461237Z","iopub.status.busy":"2023-04-18T16:06:22.460206Z","iopub.status.idle":"2023-04-18T16:06:22.473511Z","shell.execute_reply":"2023-04-18T16:06:22.472389Z","shell.execute_reply.started":"2023-04-18T16:06:22.461200Z"},"trusted":true},"outputs":[],"source":["# from transformers import BertTokenizer, BertModel\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","class ReviewDataset(Dataset):\n","    def __init__(self, df):\n","        self.df = df\n","        self.tokenizer = AutoTokenizer.from_pretrained('jackaduma/SecBERT')\n","\n","    def __getitem__(self, index):\n","        review = self.df.iloc[index][\"text\"]\n","        sentiment = self.df.iloc[index][\"label\"]\n","        sentiment_dict = {'000 - Normal': 0,\n","          '126 - Path Traversal': 1,\n","          '242 - Code Injection': 2,\n","          '153 - Input Data Manipulation': 3,\n","          '310 - Scanning for Vulnerable Software': 4,\n","          '194 - Fake the Source of Data': 5,\n","          '34 - HTTP Response Splitting': 6}\n","        label = sentiment_dict[sentiment]\n","        encoded_input = self.tokenizer.encode_plus(\n","                review,\n","                add_special_tokens=True,\n","                max_length= config[\"max_length\"],\n","                pad_to_max_length=True,\n","                return_overflowing_tokens=True,\n","            )\n","        if \"num_truncated_tokens\" in encoded_input and encoded_input[\"num_truncated_tokens\"] > 0:\n","            # print(\"Attention! you are cropping tokens\")\n","            pass\n","\n","        input_ids = encoded_input[\"input_ids\"]\n","        attention_mask = encoded_input[\"attention_mask\"] if \"attention_mask\" in encoded_input else None\n","\n","        token_type_ids = encoded_input[\"token_type_ids\"] if \"token_type_ids\" in encoded_input else None\n","\n","\n","\n","        data_input = {\n","            \"input_ids\": torch.tensor(input_ids),\n","            \"attention_mask\": torch.tensor(attention_mask),\n","            \"token_type_ids\": torch.tensor(token_type_ids),\n","            \"label\": torch.tensor(label),\n","        }\n","\n","        return data_input[\"input_ids\"], data_input[\"attention_mask\"], data_input[\"token_type_ids\"], data_input[\"label\"]\n","\n","\n","\n","    def __len__(self):\n","        return self.df.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:26.366661Z","iopub.status.busy":"2023-04-18T16:06:26.365992Z","iopub.status.idle":"2023-04-18T16:06:27.092184Z","shell.execute_reply":"2023-04-18T16:06:27.091036Z","shell.execute_reply.started":"2023-04-18T16:06:26.366623Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>GET /blog/index.php/2020/04/04/voluptatum-repr...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>GET /blog/xmlrpc.php?rsd</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GET /blog/index.php/2020/04/04/nihil-tenetur-e...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GET /blog/index.php/2020/04/04/explicabo-qui-f...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>GET /blog/index.php/2020/04/04/explicabo-qui-f...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text         label\n","0  GET /blog/index.php/2020/04/04/voluptatum-repr...  000 - Normal\n","1                           GET /blog/xmlrpc.php?rsd  000 - Normal\n","2  GET /blog/index.php/2020/04/04/nihil-tenetur-e...  000 - Normal\n","3  GET /blog/index.php/2020/04/04/explicabo-qui-f...  000 - Normal\n","4  GET /blog/index.php/2020/04/04/explicabo-qui-f...  000 - Normal"]},"execution_count":102,"metadata":{},"output_type":"execute_result"}],"source":["df_train = pd.read_csv('/kaggle/input/code-injection/dataset_capec_combine.csv')\n","df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:28.176868Z","iopub.status.busy":"2023-04-18T16:06:28.176170Z","iopub.status.idle":"2023-04-18T16:06:28.553311Z","shell.execute_reply":"2023-04-18T16:06:28.552252Z","shell.execute_reply.started":"2023-04-18T16:06:28.176822Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>GET  blog index.php 2020 04 04 voluptatum-repr...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>GET  blog xmlrpc.php?rsd</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GET  blog index.php 2020 04 04 nihil-tenetur-e...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GET  blog index.php 2020 04 04 explicabo-qui-f...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>GET  blog index.php 2020 04 04 explicabo-qui-f...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text         label\n","0  GET  blog index.php 2020 04 04 voluptatum-repr...  000 - Normal\n","1                           GET  blog xmlrpc.php?rsd  000 - Normal\n","2  GET  blog index.php 2020 04 04 nihil-tenetur-e...  000 - Normal\n","3  GET  blog index.php 2020 04 04 explicabo-qui-f...  000 - Normal\n","4  GET  blog index.php 2020 04 04 explicabo-qui-f...  000 - Normal"]},"execution_count":103,"metadata":{},"output_type":"execute_result"}],"source":["# Optional (not effect very much)\n","df_train['text'] = df_train['text'].str.replace('/',' ')\n","df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:29.715467Z","iopub.status.busy":"2023-04-18T16:06:29.715082Z","iopub.status.idle":"2023-04-18T16:06:30.161883Z","shell.execute_reply":"2023-04-18T16:06:30.160838Z","shell.execute_reply.started":"2023-04-18T16:06:29.715434Z"},"trusted":true},"outputs":[],"source":["## Reduce data for testing\n","df_242 = df_train[(df_train['label'] == '242 - Code Injection')]\n","df_242 = df_242.sample(frac = 1)\n","df_242 = df_242[:50000]\n","df_000 = df_train[(df_train['label'] == '000 - Normal')]\n","df_000 = df_000.sample(frac = 1)\n","df_000 = df_000[:50000]\n","\n","df_sub = df_train[(df_train['label'] != '000 - Normal') & (df_train['label'] != '242 - Code Injection')]\n","\n","df_train = pd.concat([df_train,df_242,df_000], ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:31.558735Z","iopub.status.busy":"2023-04-18T16:06:31.558358Z","iopub.status.idle":"2023-04-18T16:06:32.180289Z","shell.execute_reply":"2023-04-18T16:06:32.179270Z","shell.execute_reply.started":"2023-04-18T16:06:31.558701Z"},"trusted":true},"outputs":[],"source":["## prep\n","source_dataset = ReviewDataset(df_train)\n","source_dataloader = DataLoader(dataset = source_dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:32.203420Z","iopub.status.busy":"2023-04-18T16:06:32.202770Z","iopub.status.idle":"2023-04-18T16:06:32.238860Z","shell.execute_reply":"2023-04-18T16:06:32.237735Z","shell.execute_reply.started":"2023-04-18T16:06:32.203376Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>POST /vendor/phpunit/phpunit/src/Util/PHP/eval...</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>POST /cgi-bin/ViewLog.asp  remote_submit_Flag=...</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GET /.svn/wc.db</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GET /blog/.svn/wc.db</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>GET /blog/index.php/my-account/.svn/wc.db</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  \\\n","0  POST /vendor/phpunit/phpunit/src/Util/PHP/eval...   \n","1  POST /cgi-bin/ViewLog.asp  remote_submit_Flag=...   \n","2                                    GET /.svn/wc.db   \n","3                               GET /blog/.svn/wc.db   \n","4          GET /blog/index.php/my-account/.svn/wc.db   \n","\n","                           label  \n","0  153 - Input Data Manipulation  \n","1  153 - Input Data Manipulation  \n","2  153 - Input Data Manipulation  \n","3  153 - Input Data Manipulation  \n","4  153 - Input Data Manipulation  "]},"execution_count":106,"metadata":{},"output_type":"execute_result"}],"source":["df_transfer = pd.read_csv('/kaggle/input/code-injection/dataset_capec_transfer.csv')\n","df_transfer.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:32.806661Z","iopub.status.busy":"2023-04-18T16:06:32.804638Z","iopub.status.idle":"2023-04-18T16:06:32.834141Z","shell.execute_reply":"2023-04-18T16:06:32.833041Z","shell.execute_reply.started":"2023-04-18T16:06:32.806608Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>POST  vendor phpunit phpunit src Util PHP eval...</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>POST  cgi-bin ViewLog.asp  remote_submit_Flag=...</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GET  .svn wc.db</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GET  blog .svn wc.db</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>GET  blog index.php my-account .svn wc.db</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  \\\n","0  POST  vendor phpunit phpunit src Util PHP eval...   \n","1  POST  cgi-bin ViewLog.asp  remote_submit_Flag=...   \n","2                                    GET  .svn wc.db   \n","3                               GET  blog .svn wc.db   \n","4          GET  blog index.php my-account .svn wc.db   \n","\n","                           label  \n","0  153 - Input Data Manipulation  \n","1  153 - Input Data Manipulation  \n","2  153 - Input Data Manipulation  \n","3  153 - Input Data Manipulation  \n","4  153 - Input Data Manipulation  "]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["# Optional (not effect very much)\n","df_transfer['text'] = df_transfer['text'].str.replace('/',' ')\n","df_transfer.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:33.870857Z","iopub.status.busy":"2023-04-18T16:06:33.869966Z","iopub.status.idle":"2023-04-18T16:06:34.382962Z","shell.execute_reply":"2023-04-18T16:06:34.381990Z","shell.execute_reply.started":"2023-04-18T16:06:33.870818Z"},"trusted":true},"outputs":[],"source":["target_dataset = ReviewDataset(df_transfer)\n","target_dataloader = DataLoader(dataset = target_dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:36.193370Z","iopub.status.busy":"2023-04-18T16:06:36.192982Z","iopub.status.idle":"2023-04-18T16:06:36.199966Z","shell.execute_reply":"2023-04-18T16:06:36.198846Z","shell.execute_reply.started":"2023-04-18T16:06:36.193313Z"},"trusted":true},"outputs":[],"source":["from torch.autograd import Function\n","\n","\n","class GradientReversalFn(Function):\n","    @staticmethod\n","    def forward(ctx, x, alpha):\n","        ctx.alpha = alpha\n","        \n","        return x.view_as(x)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        output = grad_output.neg() * ctx.alpha\n","\n","        return output, None"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:13:38.500954Z","iopub.status.busy":"2023-04-18T16:13:38.500234Z","iopub.status.idle":"2023-04-18T16:13:38.511746Z","shell.execute_reply":"2023-04-18T16:13:38.510600Z","shell.execute_reply.started":"2023-04-18T16:13:38.500916Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","class DomainAdaptationModel(nn.Module):\n","    def __init__(self):\n","        super(DomainAdaptationModel, self).__init__()\n","        \n","        num_labels = config[\"num_labels\"]\n","        self.bert = AutoModel.from_pretrained('jackaduma/SecBERT')\n","        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n","        self.sentiment_classifier = nn.Sequential(\n","            nn.Linear(config[\"hidden_size\"], num_labels),\n","            nn.LogSoftmax(dim=1),\n","        )\n","        self.domain_classifier = nn.Sequential(\n","            nn.Linear(config[\"hidden_size\"], 2),\n","            nn.LogSoftmax(dim=1),\n","        )\n","\n","\n","    def forward(\n","          self,\n","          input_ids=None,\n","          attention_mask=None,\n","          token_type_ids=None,\n","          labels=None,\n","          grl_lambda = 1.0, \n","          ):\n","\n","        outputs = self.bert(\n","                input_ids,\n","                attention_mask=attention_mask,\n","                token_type_ids=token_type_ids,\n","            )\n","\n","#         pooled_output = outputs[1] # For bert-base-uncase\n","        pooled_output = outputs.pooler_output \n","        pooled_output = self.dropout(pooled_output)\n","\n","\n","        reversed_pooled_output = GradientReversalFn.apply(pooled_output, grl_lambda)\n","\n","        sentiment_pred = self.sentiment_classifier(pooled_output)\n","        domain_pred = self.domain_classifier(reversed_pooled_output)\n","\n","        return sentiment_pred.to(device), domain_pred.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:39.021194Z","iopub.status.busy":"2023-04-18T16:06:39.020102Z","iopub.status.idle":"2023-04-18T16:06:39.028509Z","shell.execute_reply":"2023-04-18T16:06:39.027435Z","shell.execute_reply.started":"2023-04-18T16:06:39.021153Z"},"trusted":true},"outputs":[],"source":["def compute_accuracy(logits, labels):\n","    \n","    predicted_labels_dict = {\n","      0: 0,\n","      1: 0,\n","      2: 0,\n","      3: 0,\n","      4: 0,\n","      5: 0,\n","      6: 0,\n","    }\n","    \n","    predicted_label = logits.max(dim = 1)[1]\n","    \n","    for pred in predicted_label:\n","        # print(pred.item())\n","        predicted_labels_dict[pred.item()] += 1\n","    acc = (predicted_label == labels).float().mean()\n","    \n","    return acc, predicted_labels_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:52:18.153193Z","iopub.status.busy":"2023-04-18T16:52:18.152791Z","iopub.status.idle":"2023-04-18T16:52:18.167463Z","shell.execute_reply":"2023-04-18T16:52:18.166223Z","shell.execute_reply.started":"2023-04-18T16:52:18.153158Z"},"trusted":true},"outputs":[],"source":["def evaluate(model, dataset = \"imdb\", percentage = 5):\n","    with torch.no_grad():\n","        predicted_labels_dict = {                                                   \n","          0: 0,\n","          1: 0,\n","          2: 0,\n","          3: 0,\n","          4: 0,\n","          5: 0,\n","          6: 0,                                                                   \n","        }\n","        \n","        dev_df = pd.read_csv(\"/kaggle/input/code-injection/dataset_capec_\" + dataset + \".csv\")\n","        data_size = dev_df.shape[0]\n","        selected_for_evaluation = int(data_size*percentage/100)\n","        dev_df = dev_df.head(selected_for_evaluation)\n","        dataset = ReviewDataset(dev_df)\n","\n","        dataloader = DataLoader(dataset = dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)\n","\n","        mean_accuracy = 0.0\n","        total_batches = len(dataloader)\n","        \n","        for input_ids, attention_mask, token_type_ids, labels in dataloader:\n","            inputs = {\n","                \"input_ids\": input_ids.squeeze(axis=1),\n","                \"attention_mask\": attention_mask.squeeze(axis=1),\n","                \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n","                \"labels\": labels,\n","            }\n","            for k, v in inputs.items():\n","                inputs[k] = v.to(device)\n","\n","\n","            sentiment_pred, _ = model(**inputs)\n","            accuracy, predicted_labels = compute_accuracy(sentiment_pred, inputs[\"labels\"])\n","            mean_accuracy += accuracy\n","            for i in range(7): \n","              predicted_labels_dict[i] += predicted_labels[i]\n","\n","        print(predicted_labels_dict)\n","    return mean_accuracy/total_batches"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:32:19.369124Z","iopub.status.busy":"2023-04-18T16:32:19.368752Z","iopub.status.idle":"2023-04-18T16:47:04.671093Z","shell.execute_reply":"2023-04-18T16:47:04.669299Z","shell.execute_reply.started":"2023-04-18T16:32:19.369091Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at jackaduma/SecBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Training Step: 0\n","Training Step: 5\n","Training Step: 10\n","Training Step: 15\n","Training Step: 20\n","Training Step: 25\n","Training Step: 30\n","Training Step: 35\n","Training Step: 40\n","Training Step: 45\n","Training Step: 50\n","Training Step: 55\n","Training Step: 60\n","Training Step: 65\n","Training Step: 70\n","Training Step: 75\n","Training Step: 80\n","Training Step: 85\n","Training Step: 90\n","Training Step: 95\n","Training Step: 100\n","Training Step: 105\n","Training Step: 110\n","Training Step: 115\n","Training Step: 120\n","Training Step: 125\n","Training Step: 130\n","Training Step: 135\n","Training Step: 140\n","Training Step: 145\n","Training Step: 150\n","Training Step: 155\n","Training Step: 160\n","Training Step: 165\n","Training Step: 170\n","Training Step: 175\n","Training Step: 180\n","Training Step: 185\n","Training Step: 190\n","Training Step: 195\n","Training Step: 200\n","Training Step: 205\n","Training Step: 210\n","Training Step: 215\n","Training Step: 220\n","Training Step: 225\n","Training Step: 230\n","Training Step: 235\n","Training Step: 240\n","Training Step: 245\n","Training Step: 250\n","Training Step: 255\n","Training Step: 260\n","Training Step: 265\n","Training Step: 270\n","Training Step: 275\n","Training Step: 280\n","Training Step: 285\n","Training Step: 290\n","Training Step: 295\n","Training Step: 300\n","Training Step: 305\n","Training Step: 310\n","Training Step: 315\n","Training Step: 320\n","Training Step: 325\n","Training Step: 330\n","Training Step: 335\n","Training Step: 340\n","Training Step: 345\n","Training Step: 350\n","Training Step: 355\n","Training Step: 360\n","Training Step: 365\n","Training Step: 370\n","Training Step: 375\n","Training Step: 380\n","Training Step: 385\n","Training Step: 390\n","Training Step: 395\n","Training Step: 400\n","Training Step: 405\n","Training Step: 410\n","Training Step: 415\n","Training Step: 420\n","Training Step: 425\n","Training Step: 430\n","Training Step: 435\n","Training Step: 440\n","Training Step: 445\n","Training Step: 450\n","Training Step: 455\n","Training Step: 460\n","Training Step: 465\n","Training Step: 470\n","Training Step: 475\n","Training Step: 480\n","Training Step: 485\n","Training Step: 490\n","Training Step: 495\n","Training Step: 500\n","Training Step: 505\n","Training Step: 510\n","Training Step: 515\n","Training Step: 520\n","Training Step: 525\n","Training Step: 530\n","Training Step: 535\n","Training Step: 540\n","Training Step: 545\n","Training Step: 550\n","Training Step: 555\n","Training Step: 560\n","Training Step: 565\n","Training Step: 570\n","Training Step: 575\n","Training Step: 580\n","Training Step: 585\n","Training Step: 590\n","Training Step: 595\n","Training Step: 600\n","Training Step: 605\n","Training Step: 610\n","Training Step: 615\n","Training Step: 620\n","Training Step: 625\n","Training Step: 630\n","Training Step: 635\n","Training Step: 640\n","Training Step: 645\n","Training Step: 650\n","Training Step: 655\n","Training Step: 660\n","Training Step: 665\n","Training Step: 670\n","Training Step: 675\n","Training Step: 680\n","Training Step: 685\n","Training Step: 690\n","Training Step: 695\n","Training Step: 700\n","Training Step: 705\n","Training Step: 710\n","Training Step: 715\n","Training Step: 720\n","Training Step: 725\n","Training Step: 730\n","Training Step: 735\n","Training Step: 740\n","Training Step: 745\n","Training Step: 750\n","Training Step: 755\n","Training Step: 760\n","Training Step: 765\n","Training Step: 770\n","Training Step: 775\n","Training Step: 780\n","Training Step: 785\n","Training Step: 790\n","Training Step: 795\n","Training Step: 800\n","Training Step: 805\n","Training Step: 810\n","Training Step: 815\n","Training Step: 820\n","Training Step: 825\n","Training Step: 830\n","Training Step: 835\n","Training Step: 840\n","Training Step: 845\n","Training Step: 850\n","Training Step: 855\n","Training Step: 860\n","Training Step: 865\n","Training Step: 870\n","Training Step: 875\n","Training Step: 880\n","Training Step: 885\n","Training Step: 890\n","Training Step: 895\n","Training Step: 900\n","Training Step: 905\n","Training Step: 910\n","Training Step: 915\n","Training Step: 920\n","Training Step: 925\n","Training Step: 930\n","Training Step: 935\n","Training Step: 940\n","Training Step: 945\n","Training Step: 950\n","Training Step: 955\n","Training Step: 960\n","Training Step: 965\n","Training Step: 970\n","Training Step: 975\n","Training Step: 980\n","Training Step: 985\n","Training Step: 990\n","Training Step: 995\n","Training Step: 1000\n","Training Step: 1005\n","Training Step: 1010\n","Training Step: 1015\n","Training Step: 1020\n","Training Step: 1025\n","Training Step: 1030\n","Training Step: 1035\n","Training Step: 1040\n","Training Step: 1045\n","Training Step: 1050\n","Training Step: 1055\n","Training Step: 1060\n","Training Step: 1065\n","Training Step: 1070\n","Training Step: 1075\n","Training Step: 1080\n","Training Step: 1085\n","Training Step: 1090\n","Training Step: 1095\n","Training Step: 1100\n","Training Step: 1105\n","Training Step: 1110\n","Training Step: 1115\n","Training Step: 1120\n","Training Step: 1125\n","Training Step: 1130\n","Training Step: 1135\n","Training Step: 1140\n","Training Step: 1145\n","Training Step: 1150\n","Training Step: 1155\n","Training Step: 1160\n","Training Step: 1165\n","Training Step: 1170\n","Training Step: 1175\n","Training Step: 1180\n","Training Step: 1185\n","Training Step: 1190\n","Training Step: 1195\n","Training Step: 1200\n","Training Step: 1205\n","Training Step: 1210\n","Training Step: 1215\n","Training Step: 1220\n","Training Step: 1225\n","Training Step: 1230\n","Training Step: 1235\n","Training Step: 1240\n","Training Step: 1245\n","Training Step: 1250\n","Training Step: 1255\n","Training Step: 1260\n","Training Step: 1265\n","Training Step: 1270\n","Training Step: 1275\n","Training Step: 1280\n","Training Step: 1285\n","Training Step: 1290\n","Training Step: 1295\n","Training Step: 1300\n","Training Step: 1305\n","Training Step: 1310\n","Training Step: 1315\n","Training Step: 1320\n","Training Step: 1325\n","Training Step: 1330\n","Training Step: 1335\n","Training Step: 1340\n","Training Step: 1345\n","Training Step: 1350\n","Training Step: 1355\n","Training Step: 1360\n","Training Step: 1365\n","Training Step: 1370\n","Training Step: 1375\n","Training Step: 1380\n","Training Step: 1385\n","Training Step: 1390\n","Training Step: 1395\n","Training Step: 1400\n","Training Step: 1405\n","Training Step: 1410\n","Training Step: 1415\n","Training Step: 1420\n","Training Step: 1425\n","Training Step: 1430\n","Training Step: 1435\n","Training Step: 1440\n","Training Step: 1445\n","Training Step: 1450\n","Training Step: 1455\n","Training Step: 1460\n","Training Step: 1465\n","Training Step: 1470\n","Training Step: 1475\n","Training Step: 1480\n","Training Step: 1485\n","Training Step: 1490\n","Training Step: 1495\n","Training Step: 1500\n","Training Step: 1505\n","Training Step: 1510\n","Training Step: 1515\n","Training Step: 1520\n","Training Step: 1525\n","Training Step: 1530\n","Training Step: 1535\n","Training Step: 1540\n","Training Step: 1545\n","Training Step: 1550\n","Training Step: 1555\n","Training Step: 1560\n","Training Step: 1565\n","Training Step: 1570\n","Training Step: 1575\n","Training Step: 1580\n","Training Step: 1585\n","Training Step: 1590\n","Training Step: 1595\n","Training Step: 1600\n","Training Step: 1605\n","Training Step: 1610\n","Training Step: 1615\n","Training Step: 1620\n","Training Step: 1625\n","Training Step: 1630\n","Training Step: 1635\n","Training Step: 1640\n","Training Step: 1645\n","Training Step: 1650\n","Training Step: 1655\n","Training Step: 1660\n","Training Step: 1665\n","Training Step: 1670\n","Training Step: 1675\n","Training Step: 1680\n","Training Step: 1685\n","Training Step: 1690\n","Training Step: 1695\n","Training Step: 1700\n","Training Step: 1705\n","Training Step: 1710\n","Training Step: 1715\n","Training Step: 1720\n","Training Step: 1725\n","Training Step: 1730\n","Training Step: 1735\n","Training Step: 1740\n","Training Step: 1745\n","Training Step: 1750\n","Training Step: 1755\n","Training Step: 1760\n","Training Step: 1765\n","Training Step: 1770\n","Training Step: 1775\n","Training Step: 1780\n","Training Step: 1785\n","Training Step: 1790\n","Training Step: 1795\n","Training Step: 1800\n","Training Step: 1805\n","Training Step: 1810\n","Training Step: 1815\n","Training Step: 1820\n","Training Step: 1825\n","Training Step: 1830\n","Training Step: 1835\n","Training Step: 1840\n","Training Step: 1845\n","Training Step: 1850\n","Training Step: 1855\n","Training Step: 1860\n","Training Step: 1865\n","Training Step: 1870\n","Training Step: 1875\n","Training Step: 1880\n","Training Step: 1885\n","Training Step: 1890\n","Training Step: 1895\n","Training Step: 1900\n","Training Step: 1905\n","Training Step: 1910\n","Training Step: 1915\n","Training Step: 1920\n","Training Step: 1925\n","Training Step: 1930\n","Training Step: 1935\n","Training Step: 1940\n","Training Step: 1945\n","Training Step: 1950\n","Training Step: 1955\n","Training Step: 1960\n","Training Step: 1965\n","Training Step: 1970\n","Training Step: 1975\n","Training Step: 1980\n","Training Step: 1985\n","Training Step: 1990\n","Training Step: 1995\n","Training Step: 2000\n","Training Step: 2005\n","Training Step: 2010\n","Training Step: 2015\n","Training Step: 2020\n","Training Step: 2025\n","Training Step: 2030\n","Training Step: 2035\n","Training Step: 2040\n","Training Step: 2045\n","Training Step: 2050\n","Training Step: 2055\n","Training Step: 2060\n","Training Step: 2065\n","Training Step: 2070\n","Training Step: 2075\n","Training Step: 2080\n","Training Step: 2085\n","Training Step: 2090\n","Training Step: 2095\n","Training Step: 2100\n","Training Step: 2105\n","Training Step: 2110\n","Training Step: 2115\n","Training Step: 2120\n","Training Step: 2125\n","Training Step: 2130\n","Training Step: 2135\n","Training Step: 2140\n","Training Step: 2145\n","Training Step: 2150\n","Training Step: 2155\n","Training Step: 2160\n","Training Step: 2165\n","Training Step: 2170\n","Training Step: 2175\n","Training Step: 2180\n","Training Step: 2185\n","Training Step: 2190\n","Training Step: 2195\n","Training Step: 2200\n","Training Step: 2205\n","Training Step: 2210\n","Training Step: 2215\n","Training Step: 2220\n","Training Step: 2225\n","Training Step: 2230\n","Training Step: 2235\n","Training Step: 2240\n","Training Step: 2245\n","Training Step: 2250\n","Training Step: 2255\n","Training Step: 2260\n","Training Step: 2265\n","Training Step: 2270\n","Training Step: 2275\n","Training Step: 2280\n","Training Step: 2285\n","Training Step: 2290\n","Training Step: 2295\n","Training Step: 2300\n","Training Step: 2305\n","Training Step: 2310\n","Training Step: 2315\n","Training Step: 2320\n","Training Step: 2325\n","Training Step: 2330\n","Training Step: 2335\n","Training Step: 2340\n","Training Step: 2345\n","Training Step: 2350\n","Training Step: 2355\n","Training Step: 2360\n","Training Step: 2365\n","Training Step: 2370\n","Training Step: 2375\n","Training Step: 2380\n","Training Step: 2385\n","Training Step: 2390\n","Training Step: 2395\n","Training Step: 2400\n","Training Step: 2405\n","Training Step: 2410\n","Training Step: 2415\n","Training Step: 2420\n","Training Step: 2425\n","Training Step: 2430\n","Training Step: 2435\n","Training Step: 2440\n","Training Step: 2445\n","Training Step: 2450\n","Training Step: 2455\n","Training Step: 2460\n","Training Step: 2465\n","Training Step: 2470\n","Training Step: 2475\n","Training Step: 2480\n","Training Step: 2485\n","Training Step: 2490\n","Training Step: 2495\n","Training Step: 2500\n","Training Step: 2505\n","Training Step: 2510\n","Training Step: 2515\n","Training Step: 2520\n","Training Step: 2525\n","Training Step: 2530\n","Training Step: 2535\n","Training Step: 2540\n","Training Step: 2545\n","Training Step: 2550\n","Training Step: 2555\n","Training Step: 2560\n","Training Step: 2565\n","Training Step: 2570\n","Training Step: 2575\n","Training Step: 2580\n","Training Step: 2585\n","Training Step: 2590\n","Training Step: 2595\n","Training Step: 2600\n","Training Step: 2605\n","Training Step: 2610\n","Training Step: 2615\n","Training Step: 2620\n","Training Step: 2625\n","Training Step: 2630\n","Training Step: 2635\n","Training Step: 2640\n","Training Step: 2645\n","Training Step: 2650\n","Training Step: 2655\n","Training Step: 2660\n","Training Step: 2665\n","Training Step: 2670\n","Training Step: 2675\n","Training Step: 2680\n","Training Step: 2685\n","Training Step: 2690\n","Training Step: 2695\n","Training Step: 2700\n","Training Step: 2705\n","Training Step: 2710\n","Training Step: 2715\n","Training Step: 2720\n","Training Step: 2725\n","Training Step: 2730\n","Training Step: 2735\n","Training Step: 2740\n","Training Step: 2745\n","Training Step: 2750\n","Training Step: 2755\n","Training Step: 2760\n","Training Step: 2765\n","Training Step: 2770\n","Training Step: 2775\n","Training Step: 2780\n","Training Step: 2785\n","Training Step: 2790\n","Training Step: 2795\n","Training Step: 2800\n","Training Step: 2805\n","Training Step: 2810\n","Training Step: 2815\n","Training Step: 2820\n","Training Step: 2825\n","Training Step: 2830\n","Training Step: 2835\n","Training Step: 2840\n","Training Step: 2845\n","Training Step: 2850\n","Training Step: 2855\n","Training Step: 2860\n","Training Step: 2865\n","Training Step: 2870\n","Training Step: 2875\n","Training Step: 2880\n","Training Step: 2885\n","Training Step: 2890\n","Training Step: 2895\n","Training Step: 2900\n","Training Step: 2905\n","Training Step: 2910\n","Training Step: 2915\n","Training Step: 2920\n","Training Step: 2925\n","Training Step: 2930\n","Training Step: 2935\n","Training Step: 2940\n","Training Step: 2945\n","Training Step: 2950\n","Training Step: 2955\n","Training Step: 2960\n","Training Step: 2965\n","Training Step: 2970\n","Training Step: 2975\n","Training Step: 2980\n","Training Step: 2985\n","Training Step: 2990\n","Training Step: 2995\n","Training Step: 3000\n","Training Step: 3005\n","Training Step: 3010\n","Training Step: 3015\n","Training Step: 3020\n","Training Step: 3025\n","Training Step: 3030\n","Training Step: 3035\n","Training Step: 3040\n","Training Step: 3045\n","Training Step: 3050\n","Training Step: 3055\n","Training Step: 3060\n","Training Step: 3065\n","Training Step: 3070\n","Training Step: 3075\n","Training Step: 3080\n","Training Step: 3085\n","Training Step: 3090\n","Training Step: 3095\n","Training Step: 3100\n","Training Step: 3105\n","Training Step: 3110\n","Training Step: 3115\n","Training Step: 3120\n","Training Step: 3125\n","Training Step: 3130\n","Training Step: 3135\n","Training Step: 3140\n","Training Step: 3145\n","Training Step: 3150\n","Training Step: 3155\n","Training Step: 3160\n","Training Step: 3165\n","Training Step: 3170\n","Training Step: 3175\n","Training Step: 3180\n","Training Step: 3185\n","Training Step: 3190\n","Training Step: 3195\n","Training Step: 3200\n","Training Step: 3205\n","Training Step: 3210\n","Training Step: 3215\n","Training Step: 3220\n","Training Step: 3225\n","Training Step: 3230\n","Training Step: 3235\n","Training Step: 3240\n","Training Step: 3245\n","Training Step: 3250\n","Training Step: 3255\n","Training Step: 3260\n","Training Step: 3265\n","Training Step: 3270\n","Training Step: 3275\n","Training Step: 3280\n","Training Step: 3285\n","Training Step: 3290\n","Training Step: 3295\n","Training Step: 3300\n","Training Step: 3305\n","Training Step: 3310\n","Training Step: 3315\n","Training Step: 3320\n","Training Step: 3325\n","Training Step: 3330\n","Training Step: 3335\n","Training Step: 3340\n","Training Step: 3345\n","Training Step: 3350\n","Training Step: 3355\n","Training Step: 3360\n","Training Step: 3365\n","Training Step: 3370\n","Training Step: 3375\n","Training Step: 3380\n","Training Step: 3385\n","Training Step: 3390\n","Training Step: 3395\n","Training Step: 3400\n","Training Step: 3405\n","Training Step: 3410\n","Training Step: 3415\n","Training Step: 3420\n","Training Step: 3425\n","Training Step: 3430\n","Training Step: 3435\n","Training Step: 3440\n","Training Step: 3445\n","Training Step: 3450\n","Training Step: 3455\n","Training Step: 3460\n","Training Step: 3465\n","Training Step: 3470\n","Training Step: 3475\n","Training Step: 3480\n","Training Step: 3485\n","Training Step: 3490\n","Training Step: 3495\n","Training Step: 3500\n","Training Step: 3505\n","Training Step: 3510\n","Training Step: 3515\n","Training Step: 3520\n","Training Step: 3525\n","Training Step: 3530\n","Training Step: 3535\n","Training Step: 3540\n","Training Step: 3545\n","Training Step: 3550\n","Training Step: 3555\n","Training Step: 3560\n","Training Step: 3565\n","Training Step: 3570\n","Training Step: 3575\n","Training Step: 3580\n","Training Step: 3585\n","Training Step: 3590\n","Training Step: 3595\n","Training Step: 3600\n","Training Step: 3605\n","Training Step: 3610\n","Training Step: 3615\n","Training Step: 3620\n","Training Step: 3625\n","Training Step: 3630\n","Training Step: 3635\n","Training Step: 3640\n","Training Step: 3645\n","Training Step: 3650\n","Training Step: 3655\n","Training Step: 3660\n","Training Step: 3665\n","Training Step: 3670\n","Training Step: 3675\n","Training Step: 3680\n","Training Step: 3685\n","Training Step: 3690\n","Training Step: 3695\n","Training Step: 3700\n","Training Step: 3705\n","Training Step: 3710\n","Training Step: 3715\n","Training Step: 3720\n","Training Step: 3725\n","Training Step: 3730\n","Training Step: 3735\n","Training Step: 3740\n","Training Step: 3745\n","Training Step: 3750\n","Training Step: 3755\n","Training Step: 3760\n","Training Step: 3765\n","Training Step: 3770\n","Training Step: 3775\n","Training Step: 3780\n","Training Step: 3785\n","Training Step: 3790\n","Training Step: 3795\n","Training Step: 3800\n","Training Step: 3805\n","Training Step: 3810\n","Training Step: 3815\n","Training Step: 3820\n","Training Step: 3825\n","Training Step: 3830\n","Training Step: 3835\n","Training Step: 3840\n","Training Step: 3845\n","Training Step: 3850\n","Training Step: 3855\n","Training Step: 3860\n","Training Step: 3865\n","Training Step: 3870\n","Training Step: 3875\n","Training Step: 3880\n","Training Step: 3885\n","Training Step: 3890\n","Training Step: 3895\n","Training Step: 3900\n","Training Step: 3905\n","Training Step: 3910\n","Training Step: 3915\n","Training Step: 3920\n","Training Step: 3925\n","Training Step: 3930\n","Training Step: 3935\n","Training Step: 3940\n","Training Step: 3945\n","Training Step: 3950\n","Training Step: 3955\n","Training Step: 3960\n","Training Step: 3965\n","Training Step: 3970\n","Training Step: 3975\n","Training Step: 3980\n","Training Step: 3985\n","Training Step: 3990\n","Training Step: 3995\n","Training Step: 4000\n","Training Step: 4005\n","Training Step: 4010\n","Training Step: 4015\n","Training Step: 4020\n","Training Step: 4025\n","Training Step: 4030\n","Training Step: 4035\n","Training Step: 4040\n","Training Step: 4045\n","Training Step: 4050\n","Training Step: 4055\n","Training Step: 4060\n","Training Step: 4065\n","Training Step: 4070\n","Training Step: 4075\n","Training Step: 4080\n","Training Step: 4085\n","Training Step: 4090\n","Training Step: 4095\n","Training Step: 4100\n","Training Step: 4105\n","Training Step: 4110\n","Training Step: 4115\n","Training Step: 4120\n","Training Step: 4125\n","Training Step: 4130\n","Training Step: 4135\n","Training Step: 4140\n","Training Step: 4145\n","Training Step: 4150\n","Training Step: 4155\n","Training Step: 4160\n","Training Step: 4165\n","Training Step: 4170\n","Training Step: 4175\n","Training Step: 4180\n","Training Step: 4185\n","Training Step: 4190\n","Training Step: 4195\n","Training Step: 4200\n","Training Step: 4205\n","Training Step: 4210\n","Training Step: 4215\n","Training Step: 4220\n","Training Step: 4225\n","Training Step: 4230\n","Training Step: 4235\n","Training Step: 4240\n","Training Step: 4245\n","Training Step: 4250\n","Training Step: 4255\n","Training Step: 4260\n","Training Step: 4265\n","Training Step: 4270\n","Training Step: 4275\n","Training Step: 4280\n","Training Step: 4285\n","Training Step: 4290\n","Training Step: 4295\n","Training Step: 4300\n","Training Step: 4305\n","Training Step: 4310\n","Training Step: 4315\n","Training Step: 4320\n","Training Step: 4325\n","Training Step: 4330\n","Training Step: 4335\n","Training Step: 4340\n","Training Step: 4345\n","Training Step: 4350\n","Training Step: 4355\n","Training Step: 4360\n","Training Step: 4365\n","Training Step: 4370\n","Training Step: 4375\n","Training Step: 4380\n","Training Step: 4385\n","Training Step: 4390\n","Training Step: 4395\n","Training Step: 4400\n","Training Step: 4405\n","Training Step: 4410\n","Training Step: 4415\n","Training Step: 4420\n","Training Step: 4425\n","Training Step: 4430\n","Training Step: 4435\n","Training Step: 4440\n","Training Step: 4445\n","Training Step: 4450\n","Training Step: 4455\n","Training Step: 4460\n","Training Step: 4465\n","Training Step: 4470\n","Training Step: 4475\n","Training Step: 4480\n","Training Step: 4485\n","Training Step: 4490\n","Training Step: 4495\n","Training Step: 4500\n","Training Step: 4505\n","Training Step: 4510\n","Training Step: 4515\n","Training Step: 4520\n","Training Step: 4525\n","Training Step: 4530\n","Training Step: 4535\n","Training Step: 4540\n","Training Step: 4545\n","Training Step: 4550\n","Training Step: 4555\n","Training Step: 4560\n","Training Step: 4565\n","Training Step: 4570\n","Training Step: 4575\n","Training Step: 4580\n","Training Step: 4585\n","Training Step: 4590\n","Training Step: 4595\n","Training Step: 4600\n","Training Step: 4605\n","Training Step: 4610\n","Training Step: 4615\n","Training Step: 4620\n","Training Step: 4625\n","Training Step: 4630\n","Training Step: 4635\n","Training Step: 4640\n","Training Step: 4645\n","Training Step: 4650\n","Training Step: 4655\n","Training Step: 4660\n","Training Step: 4665\n","Training Step: 4670\n","Training Step: 4675\n","Training Step: 4680\n","Training Step: 4685\n","Training Step: 4690\n","Training Step: 4695\n","Training Step: 4700\n","Training Step: 4705\n","Training Step: 4710\n","Training Step: 4715\n","Training Step: 4720\n","Training Step: 4725\n","Training Step: 4730\n","Training Step: 4735\n","Training Step: 4740\n","Training Step: 4745\n","Training Step: 4750\n","Training Step: 4755\n","Training Step: 4760\n","Training Step: 4765\n","Training Step: 4770\n","Training Step: 4775\n","Training Step: 4780\n","Training Step: 4785\n","Training Step: 4790\n","Training Step: 4795\n","Training Step: 4800\n","Training Step: 4805\n","Training Step: 4810\n","Training Step: 4815\n","Training Step: 4820\n","Training Step: 4825\n","Training Step: 4830\n","Training Step: 4835\n","Training Step: 4840\n","Training Step: 4845\n","Training Step: 4850\n","Training Step: 4855\n","Training Step: 4860\n","Training Step: 4865\n","Training Step: 4870\n","Training Step: 4875\n","Training Step: 4880\n","Training Step: 4885\n","Training Step: 4890\n","Training Step: 4895\n","Training Step: 4900\n","Training Step: 4905\n","Training Step: 4910\n","Training Step: 4915\n","Training Step: 4920\n","Training Step: 4925\n","Training Step: 4930\n","Training Step: 4935\n","Training Step: 4940\n","Training Step: 4945\n","Training Step: 4950\n","Training Step: 4955\n","Training Step: 4960\n","Training Step: 4965\n","Training Step: 4970\n","Training Step: 4975\n","Training Step: 4980\n","Training Step: 4985\n","Training Step: 4990\n","Training Step: 4995\n","Training Step: 5000\n","Training Step: 5005\n","Training Step: 5010\n","Training Step: 5015\n","Training Step: 5020\n","Training Step: 5025\n","Training Step: 5030\n","Training Step: 5035\n","Training Step: 5040\n","Training Step: 5045\n","Training Step: 5050\n","Training Step: 5055\n","Training Step: 5060\n","Training Step: 5065\n","Training Step: 5070\n","Training Step: 5075\n","Training Step: 5080\n","Training Step: 5085\n","Training Step: 5090\n","Training Step: 5095\n","Training Step: 5100\n","Training Step: 5105\n","Training Step: 5110\n","Training Step: 5115\n","Training Step: 5120\n","Training Step: 5125\n","Training Step: 5130\n","Training Step: 5135\n","Training Step: 5140\n","Training Step: 5145\n","Training Step: 5150\n","Training Step: 5155\n","Training Step: 5160\n","Training Step: 5165\n","Training Step: 5170\n","Training Step: 5175\n","Training Step: 5180\n","Training Step: 5185\n","Training Step: 5190\n","Training Step: 5195\n","Training Step: 5200\n","Training Step: 5205\n","Training Step: 5210\n","Training Step: 5215\n","Training Step: 5220\n","Training Step: 5225\n","Training Step: 5230\n","Training Step: 5235\n","Training Step: 5240\n","Training Step: 5245\n","Training Step: 5250\n","Training Step: 5255\n","Training Step: 5260\n","Training Step: 5265\n","Training Step: 5270\n","Training Step: 5275\n","Training Step: 5280\n","Training Step: 5285\n","Training Step: 5290\n","Training Step: 5295\n","Training Step: 5300\n","Training Step: 5305\n","Training Step: 5310\n","Training Step: 5315\n","Training Step: 5320\n","Training Step: 5325\n","Training Step: 5330\n","Training Step: 5335\n","Training Step: 5340\n","Training Step: 5345\n","Training Step: 5350\n","Training Step: 5355\n","Training Step: 5360\n","Training Step: 5365\n","Training Step: 5370\n","Training Step: 5375\n","Training Step: 5380\n","Training Step: 5385\n","Training Step: 5390\n","Training Step: 5395\n","Training Step: 5400\n","Training Step: 5405\n","Training Step: 5410\n","Training Step: 5415\n","Training Step: 5420\n","Training Step: 5425\n","Training Step: 5430\n","Training Step: 5435\n","Training Step: 5440\n","Training Step: 5445\n","Training Step: 5450\n","Training Step: 5455\n","Training Step: 5460\n","Training Step: 5465\n","Training Step: 5470\n","Training Step: 5475\n","Training Step: 5480\n","Training Step: 5485\n","Training Step: 5490\n","Training Step: 5495\n","Training Step: 5500\n","Training Step: 5505\n","Training Step: 5510\n","Training Step: 5515\n","Training Step: 5520\n","Training Step: 5525\n","Training Step: 5530\n","Training Step: 5535\n","Training Step: 5540\n","Training Step: 5545\n","Training Step: 5550\n","Training Step: 5555\n","Training Step: 5560\n","Training Step: 5565\n","Training Step: 5570\n","Training Step: 5575\n","Training Step: 5580\n","Training Step: 5585\n","Training Step: 5590\n","Training Step: 5595\n","Training Step: 5600\n","Training Step: 5605\n","Training Step: 5610\n","Training Step: 5615\n","Training Step: 5620\n","Training Step: 5625\n","Training Step: 5630\n","Training Step: 5635\n","Training Step: 5640\n","Training Step: 5645\n","Training Step: 5650\n","Training Step: 5655\n","Training Step: 5660\n","Training Step: 5665\n","Training Step: 5670\n","Training Step: 5675\n","Training Step: 5680\n","Training Step: 5685\n","Training Step: 5690\n","Training Step: 5695\n","Training Step: 5700\n","Training Step: 5705\n","Training Step: 5710\n","Training Step: 5715\n","Training Step: 5720\n","Training Step: 5725\n","Training Step: 5730\n","Training Step: 5735\n","Training Step: 5740\n","Training Step: 5745\n","Training Step: 5750\n","Training Step: 5755\n","Training Step: 5760\n","Training Step: 5765\n","Training Step: 5770\n","Training Step: 5775\n","Training Step: 5780\n","Training Step: 5785\n","Training Step: 5790\n","Training Step: 5795\n","Training Step: 5800\n","Training Step: 5805\n","Training Step: 5810\n","Training Step: 5815\n","Training Step: 5820\n","Training Step: 5825\n","Training Step: 5830\n","Training Step: 5835\n","Training Step: 5840\n","Training Step: 5845\n","Training Step: 5850\n","Training Step: 5855\n","Training Step: 5860\n","Training Step: 5865\n","Training Step: 5870\n","Training Step: 5875\n","Training Step: 5880\n","Training Step: 5885\n","Training Step: 5890\n","Training Step: 5895\n","Training Step: 5900\n","Training Step: 5905\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"ename":"ValueError","evalue":"too many values to unpack (expected 2)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1605493686.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m#     print(\"Accuracy on amazon after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transfer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on transfer dataset after epoch \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/3521299415.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dataset, percentage)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0msentiment_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mmean_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/2504870558.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, labels, grl_lambda)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}],"source":["lr = training_parameters[\"learning_rate\"]\n","n_epochs = training_parameters[\"epochs\"]\n","\n","model = DomainAdaptationModel()\n","model.to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr)\n","\n","loss_fn_sentiment_classifier = torch.nn.NLLLoss()\n","loss_fn_domain_classifier = torch.nn.NLLLoss()\n","'''\n","In one training step we will update the model using both the source labeled data and target unlabeled data\n","We will run it till the batches last for any of these datasets\n","\n","In our case target dataset has more data. Hence, we will leverage the entire source dataset for training\n","\n","If we use the same approach in a case where the source dataset has more data then the target dataset then we will\n","under-utilize the labeled source dataset. In such a scenario it is better to reload the target dataset when it finishes\n","This will ensure that we are utilizing the entire source dataset to train our model.\n","'''\n","\n","max_batches = min(len(source_dataloader), len(target_dataloader))\n","\n","for epoch_idx in range(n_epochs):\n","    \n","    source_iterator = iter(source_dataloader)\n","    target_iterator = iter(target_dataloader)\n","\n","    for batch_idx in range(max_batches):\n","        \n","        p = float(batch_idx + epoch_idx * max_batches) / (training_parameters[\"epochs\"] * max_batches)\n","        grl_lambda = 2. / (1. + np.exp(-10 * p)) - 1\n","        grl_lambda = torch.tensor(grl_lambda)\n","        \n","        model.train()\n","        \n","        if(batch_idx%training_parameters[\"print_after_steps\"] == 0 ):\n","            print(\"Training Step:\", batch_idx)\n","        \n","        optimizer.zero_grad()\n","        \n","        # Souce dataset training update\n","        input_ids, attention_mask, token_type_ids, labels = next(source_iterator)\n","        inputs = {\n","            \"input_ids\": input_ids.squeeze(axis=1),\n","            \"attention_mask\": attention_mask.squeeze(axis=1),\n","            \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n","            \"labels\" : labels,\n","            \"grl_lambda\" : grl_lambda,\n","        }\n","\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","    \n","        sentiment_pred, domain_pred = model(**inputs)\n","        loss_s_sentiment = loss_fn_sentiment_classifier(sentiment_pred, inputs[\"labels\"])\n","        y_s_domain = torch.zeros(training_parameters[\"batch_size\"], dtype=torch.long).to(device)\n","        loss_s_domain = loss_fn_domain_classifier(domain_pred, y_s_domain)\n","\n","\n","        # Target dataset training update \n","        input_ids, attention_mask, token_type_ids, labels = next(target_iterator)\n","        inputs = {\n","            \"input_ids\": input_ids.squeeze(axis=1),\n","            \"attention_mask\": attention_mask.squeeze(axis=1),\n","            \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n","            \"labels\" : labels,\n","            \"grl_lambda\" : grl_lambda,\n","        }\n","\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","    \n","        _, domain_pred = model(**inputs)\n","        \n","        # Note that we are not using the sentiment predictions here for updating the weights\n","        y_t_domain = torch.ones(input_ids.shape[0], dtype=torch.long).to(device)\n","        # print(domain_pred.shape, y_t_domain.shape)\n","        loss_t_domain = loss_fn_domain_classifier(domain_pred, y_t_domain)\n","\n","        # Combining the loss \n","\n","        loss = loss_s_sentiment + loss_s_domain + loss_t_domain\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Evaluate the model after every epoch\n","    \n","    torch.save(model.state_dict(), os.path.join(training_parameters[\"output_folder\"], \"epoch_\" + str(epoch_idx)  +  training_parameters[\"output_file\"] ))\n","#     accuracy = evaluate(model, dataset = \"combine\", percentage = 1).item()\n","#     print(\"Accuracy on amazon after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))\n","\n","    accuracy = evaluate(model, dataset = \"transfer\", percentage = 100).item()\n","    print(\"Accuracy on transfer dataset after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:52:32.160221Z","iopub.status.busy":"2023-04-18T16:52:32.159031Z","iopub.status.idle":"2023-04-18T16:54:51.158166Z","shell.execute_reply":"2023-04-18T16:54:51.157030Z","shell.execute_reply.started":"2023-04-18T16:52:32.160172Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2352: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","{0: 3802, 1: 442, 2: 6538, 3: 1, 4: 28, 5: 59, 6: 949}\n","Accuracy on transfer dataset after epoch 0 is 0.5217428207397461\n"]}],"source":["    accuracy = evaluate(model, dataset = \"transfer\", percentage = 100).item()\n","    print(\"Accuracy on transfer dataset after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
