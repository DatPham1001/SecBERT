{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hl100\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from scipy.stats import chi2\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for training, you should modify these values to get the best performance\n",
    "config = {\n",
    "    \"num_labels\": 7,\n",
    "    \"hidden_dropout_prob\": 0.15,\n",
    "    \"hidden_size\": 768,\n",
    "    \"max_length\": 512,\n",
    "}\n",
    "\n",
    "training_parameters = {\n",
    "    \"batch_size\": 10,\n",
    "    \"epochs\": 2,\n",
    "    \"output_folder\": \"/colab/working\",\n",
    "    \"output_file\": \"model.bin\",\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"print_after_steps\": 100,\n",
    "    \"save_steps\": 5000,\n",
    "\n",
    "}\n",
    "predicted_labels_dict = {\n",
    "    0: 0,\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 0,\n",
    "    4: 0,\n",
    "    5: 0,\n",
    "    6: 0,\n",
    "    7: 0\n",
    "}\n",
    "# 000 - Normal\n",
    "# 126 - Path Traversal\n",
    "# 66 - SQL Injection\n",
    "# 272 - Protocol Manipulation\n",
    "# 310 - Scanning for Vulnerable Software\n",
    "# 242 - Code Injection\n",
    "# 194 - Fake the Source of Data\n",
    "# 34 - HTTP Response Splitting\n",
    "# 153 - Input Data Manipulation\n",
    "\n",
    "attack_dict = {\n",
    "    '000 - Normal': 0,\n",
    "    '126 - Path Traversal': 1,\n",
    "    '242 - Code Injection': 2,\n",
    "    '153 - Input Data Manipulation': 3,\n",
    "    '310 - Scanning for Vulnerable Software': 4,\n",
    "    '194 - Fake the Source of Data': 5,\n",
    "    '34 - HTTP Response Splitting': 6,\n",
    "    '66 - SQL Injection':7,\n",
    "    '272 - Protocol Manipulation':8\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('jackaduma/secBERT')\n",
    "    def __getitem__(self, index):\n",
    "        review = self.df.iloc[index][\"text\"]\n",
    "        attack = self.df.iloc[index][\"label\"]\n",
    "\n",
    "        label = attack_dict[attack]\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "                review,\n",
    "                add_special_tokens=True,\n",
    "                max_length = 512,\n",
    "                padding=\"max_length\",\n",
    "                return_overflowing_tokens=True,\n",
    "                truncation = True,\n",
    "            )\n",
    "        if \"num_truncated_tokens\" in encoded_input and encoded_input[\"num_truncated_tokens\"] > 0:\n",
    "            # print(\"Attention! you are cropping tokens\")\n",
    "            pass\n",
    "\n",
    "        input_ids = encoded_input[\"input_ids\"]\n",
    "        attention_mask = encoded_input[\"attention_mask\"] if \"attention_mask\" in encoded_input else None\n",
    "\n",
    "        token_type_ids = encoded_input[\"token_type_ids\"] if \"token_type_ids\" in encoded_input else None\n",
    "\n",
    "\n",
    "\n",
    "        data_input = {\n",
    "            \"input_ids\": torch.tensor(input_ids),\n",
    "            \"attention_mask\": torch.tensor(attention_mask),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids),\n",
    "            \"label\": torch.tensor(label),\n",
    "        }\n",
    "\n",
    "        return data_input[\"input_ids\"], data_input[\"attention_mask\"], data_input[\"token_type_ids\"], data_input[\"label\"]\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\H'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\H'\n",
      "C:\\Users\\hl100\\AppData\\Local\\Temp\\ipykernel_9904\\2045137152.py:1: SyntaxWarning: invalid escape sequence '\\H'\n",
      "  df_full = pd.read_csv('D:\\Hoc\\SecBert\\SecBERT\\multilabel-train\\dataset_capec.csv');\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "000 - Normal                              30\n",
      "126 - Path Traversal                      30\n",
      "66 - SQL Injection                        30\n",
      "272 - Protocol Manipulation               30\n",
      "310 - Scanning for Vulnerable Software    30\n",
      "242 - Code Injection                      30\n",
      "153 - Input Data Manipulation             30\n",
      "194 - Fake the Source of Data             30\n",
      "34 - HTTP Response Splitting              30\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_full = pd.read_csv('D:\\Hoc\\SecBert\\SecBERT\\multilabel-train\\dataset_capec.csv');\n",
    "df_full['text'] = df_full['text'].str.replace('/',' ')\n",
    "df_full.head(5)\n",
    "\n",
    "# df_sub = df_train[(df_train['label'] != '000 - Normal') & (df_train['label'] != '242 - Code Injection')]\n",
    "df_train = df_full.groupby('label').head(30)\n",
    "print(df_train['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hl100\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "## prepare for training\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_train['text'], df_train['label'],test_size=0.3, stratify=df_train['label'], shuffle = True)\n",
    "df_train = pd.concat([X_train, Y_train], axis=1)\n",
    "df_test = pd.concat([X_test, Y_test], axis=1)\n",
    "df_train = df_train[0:len(df_train)//training_parameters['batch_size']*training_parameters['batch_size']]\n",
    "source_dataset = ReviewDataset(df_train)\n",
    "source_dataloader = DataLoader(dataset = source_dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Function\n",
    "class DomainAdaptationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainAdaptationModel, self).__init__()\n",
    "        num_labels = config[\"num_labels\"]\n",
    "        self.bert = AutoModel.from_pretrained('jackaduma/SecBERT')\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(config[\"hidden_size\"], num_labels),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(config[\"hidden_size\"], 2),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "          self,\n",
    "          input_ids=None,\n",
    "          attention_mask=None,\n",
    "          token_type_ids=None,\n",
    "          labels=None,\n",
    "          grl_lambda = 1.0,\n",
    "          ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "            )\n",
    "\n",
    "#         pooled_output = outputs[1] # For bert-base-uncase\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "\n",
    "        reversed_pooled_output = GradientReversalFn.apply(pooled_output, grl_lambda)\n",
    "\n",
    "        sentiment_pred = self.sentiment_classifier(pooled_output)\n",
    "        domain_pred = self.domain_classifier(reversed_pooled_output)\n",
    "\n",
    "        return sentiment_pred.to(device), domain_pred.to(device)\n",
    "\n",
    "\n",
    "class GradientReversalFn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "\n",
    "        return output, None\n",
    "    \n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "\n",
    "    predicted_label = logits.max(dim = 1)[1]\n",
    "\n",
    "    for pred in predicted_label:\n",
    "        predicted_labels_dict[pred.item()] += 1\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "\n",
    "    return acc, predicted_labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report,accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "def evaluate(model):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        dataset = ReviewDataset(df_test)\n",
    "        dataloader = DataLoader(dataset = dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)\n",
    "\n",
    "        true_labels = list()\n",
    "        predicted_label = list()\n",
    "        for input_ids, attention_mask, token_type_ids, labels in dataloader:\n",
    "            inputs = {\n",
    "                \"input_ids\": input_ids.squeeze(axis=1),\n",
    "                \"attention_mask\": attention_mask.squeeze(axis=1),\n",
    "                \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n",
    "                \"labels\": labels,\n",
    "            }\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(device)\n",
    "            attack_pred, _ = model(**inputs)\n",
    "            true_labels.extend(inputs['labels'].cpu().numpy())\n",
    "            predicted_label.extend(attack_pred.max(dim = 1)[1].cpu().numpy())\n",
    "            _, predicted_labels = compute_accuracy(attack_pred, inputs[\"labels\"])\n",
    "\n",
    "            for i in range(7):\n",
    "                  predicted_labels_dict[i] += predicted_labels[i]\n",
    "\n",
    "        score = f1_score(true_labels,predicted_label,average=\"macro\")\n",
    "        precision = precision_score(true_labels, predicted_label,average=\"macro\")\n",
    "        recall = recall_score(true_labels, predicted_label,average=\"macro\")\n",
    "        report = classification_report(true_labels,predicted_label,digits=4)\n",
    "        acc= accuracy_score(true_labels, predicted_label)\n",
    "        #classifaction_report_csv(report,precision,recall   ,score,0)\n",
    "        print ('\\n clasification report:\\n', report)\n",
    "        print ('F1 score:', score)\n",
    "        print ('Recall:', recall)\n",
    "        print ('Precision:', precision)\n",
    "        print ('Acc:', acc)\n",
    "        print('Confusion Matrix: \\n',confusion_matrix(true_labels, predicted_label))\n",
    "        print(predicted_labels_dict)\n",
    "    print(\"Testing time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_batches: 18\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "lr = training_parameters[\"learning_rate\"]\n",
    "n_epochs = training_parameters[\"epochs\"]\n",
    "\n",
    "model = DomainAdaptationModel()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "loss_fn_attack_classifier = torch.nn.NLLLoss(ignore_index=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "max_batches = len(source_dataloader)\n",
    "print(\"max_batches:\", max_batches)\n",
    "for epoch_idx in range(2):\n",
    "    source_iterator = iter(source_dataloader)\n",
    "    for batch_idx in range(max_batches):\n",
    "        print(\"batch_idx:\", batch_idx)\n",
    "        model.train()\n",
    "        if(batch_idx%training_parameters[\"print_after_steps\"] == 0 ):\n",
    "            print(\"Training Step:\", batch_idx)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Souce dataset training update\n",
    "        input_ids, attention_mask, token_type_ids, labels = next(source_iterator)\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids.squeeze(axis=1),\n",
    "            \"attention_mask\": attention_mask.squeeze(axis=1),\n",
    "            \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n",
    "            \"labels\" : labels,\n",
    "        }\n",
    "\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        attack_pred, pooled_output_prj_source = model(**inputs)\n",
    "        loss_s_attack = loss_fn_attack_classifier(attack_pred, inputs[\"labels\"])\n",
    "        # loss_s_attack = loss_fn_attack_classifier(attack_pred, torch.clamp(inputs[\"labels\"], 0, attack_pred.shape[1]-1))\n",
    "\n",
    "        loss = loss_s_attack\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: \" + str(epoch_idx))\n",
    "print(\"Training time:\", time.time()-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
