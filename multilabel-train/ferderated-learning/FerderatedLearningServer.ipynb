{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from scipy.stats import chi2\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Function\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from centralized import DomainAdaptationModel,ReviewDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainAdaptationClient(fl.client.NumPyClient):\n",
    "    def __init__(self, model, train_loader, test_loader, device):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "\n",
    "    # def get_parameters(self):\n",
    "    #     return [val.cpu().numpy() for val in self.model.state_dict().values()]\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        state_dict = {k: torch.tensor(v) for k, v in zip(self.model.state_dict().keys(), parameters)}\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        print(\"Starting training...\")\n",
    "        self.model.train()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        for epoch in range(1):  # Perform a single epoch of training\n",
    "            for batch_idx,batch in self.train_loader:\n",
    "                input_ids, attention_mask, token_type_ids, labels = [x.to(self.device) for x in batch]\n",
    "                sentiment_pred, domain_pred = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                loss = self.compute_loss(sentiment_pred, domain_pred, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                  # Calculate loss\n",
    "                total_loss += loss.item()\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(sentiment_pred, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"Epoch [{epoch+1}/{1}], Batch [{batch_idx+1}/{len(self.train_loader)}], \"\n",
    "                          f\"Loss: {loss.item():.4f}, Accuracy: {correct/total:.4f}\")\n",
    "         # Print final accuracy after epoch\n",
    "        accuracy = correct / total\n",
    "        print(f\"Training complete. Accuracy: {accuracy:.4f}, Average Loss: {total_loss/len(self.train_loader):.4f}\")\n",
    "        return self.get_parameters(), len(self.train_loader.dataset), {}\n",
    "\n",
    "    def compute_loss(self, sentiment_pred, domain_pred, labels):\n",
    "        sentiment_loss = torch.nn.CrossEntropyLoss()(sentiment_pred, labels)\n",
    "        domain_loss = torch.nn.CrossEntropyLoss()(domain_pred, labels)  # Assuming domain labels are in labels\n",
    "        return sentiment_loss + domain_loss\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        print(\"Starting evaluation...\")\n",
    "        with torch.no_grad():\n",
    "            for batch in self.test_loader:\n",
    "                input_ids, attention_mask, token_type_ids, labels = [x.to(self.device) for x in batch]\n",
    "                sentiment_pred, domain_pred = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                _, predicted = torch.max(sentiment_pred, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "        return float(accuracy), len(self.test_loader.dataset),{\"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: DeprecationWarning: invalid escape sequence '\\W'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "label\n",
      "000 - Normal                              30\n",
      "126 - Path Traversal                      30\n",
      "66 - SQL Injection                        30\n",
      "272 - Protocol Manipulation               30\n",
      "310 - Scanning for Vulnerable Software    30\n",
      "242 - Code Injection                      30\n",
      "153 - Input Data Manipulation             30\n",
      "194 - Fake the Source of Data             30\n",
      "34 - HTTP Response Splitting              30\n",
      "Name: count, dtype: int64\n",
      "540\n"
     ]
    }
   ],
   "source": [
    "#Model\n",
    "model = DomainAdaptationModel()\n",
    "tokenizer = AutoTokenizer.from_pretrained('jackaduma/SecBERT')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "# state_dict_path = 'C:/Users/hl100/Downloads/' + 'size250k_1epoch_1_model.bin'\n",
    "# model.load_state_dict(torch.load(state_dict_path, map_location=device))\n",
    "#Data\n",
    "# df_full = pd.read_csv('D:\\Hoc\\SecBert\\SecBERT\\multilabel-train\\dataset_capec.csv')\n",
    "df_full = pd.read_csv('E:\\Work_DatPT\\Study\\Master\\SecBERT\\dataset_capec.csv')\n",
    "df_full['text'] = df_full['text'].str.replace('/',' ')\n",
    "df_train = df_full.groupby('label').head(30)\n",
    "# df_train = df_full\n",
    "df_train = df_train.dropna(subset=['label'])\n",
    "label_counts = df_train['label'].value_counts()\n",
    "print(label_counts)\n",
    "print(df_train.size)\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "train_texts = train_df['text'].values\n",
    "train_labels = train_df['label'].values\n",
    "test_texts = test_df['text'].values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_train['text'], df_train['label'],test_size=0.3, stratify=df_train['label'], shuffle = True)\n",
    "df_train = pd.concat([X_train, Y_train], axis=1)\n",
    "df_test = pd.concat([X_test, Y_test], axis=1)\n",
    "\n",
    "# Tokenize the loaded texts for training and testing\n",
    "train_dataset = ReviewDataset(df_train)\n",
    "test_dataset = ReviewDataset(df_test)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. \n",
      "\tInstead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: \n",
      "\tflwr.client.start_client(\n",
      "\t\tserver_address='<IP>:<PORT>',\n",
      "\t\tclient=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object\n",
      "\t)\n",
      "\tUsing `start_numpy_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: get_parameters message 5a5935b3-3494-4770-bbec-4dec3fcb67f7\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      Disconnect and shut down\n"
     ]
    }
   ],
   "source": [
    "# Simulate clients\n",
    "client = DomainAdaptationClient(model, train_loader, test_loader, device)\n",
    "fl.client.start_numpy_client(server_address=\"localhost:8088\", client=client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
